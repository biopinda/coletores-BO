# Documenta√ß√£o T√©cnica: Integra√ß√£o BERT NER

## üìë √çndice

- [Vis√£o Geral](#vis√£o-geral)
- [Arquitetura da Solu√ß√£o](#arquitetura-da-solu√ß√£o)
- [Especifica√ß√µes do Modelo](#especifica√ß√µes-do-modelo)
- [Integra√ß√£o no Pipeline](#integra√ß√£o-no-pipeline)
- [Implementa√ß√£o T√©cnica](#implementa√ß√£o-t√©cnica)
- [Performance e Otimiza√ß√£o](#performance-e-otimiza√ß√£o)
- [Configura√ß√£o e Uso](#configura√ß√£o-e-uso)
- [M√©tricas e Monitoramento](#m√©tricas-e-monitoramento)
- [Troubleshooting](#troubleshooting)
- [Refer√™ncias](#refer√™ncias)

---

## Vis√£o Geral

### Objetivo

O m√≥dulo BERT NER (Named Entity Recognition) atua como **fallback inteligente** no est√°gio de classifica√ß√£o do pipeline, ativado automaticamente quando:

1. A classifica√ß√£o baseada em regras retorna **confian√ßa < 0.70**
2. Strings complexas ou amb√≠guas que n√£o correspondem a padr√µes conhecidos
3. M√∫ltiplas entidades n√£o identificadas por heur√≠sticas tradicionais

### Problema Resolvido

Em bancos de dados de herb√°rios, aproximadamente **0.5-2%** dos registros apresentam formatos complexos ou amb√≠guos:

```
Exemplos de casos dif√≠ceis:
- "Cc. Oliveira, L. S Inocencio, Mj. Silva N. Carvalho, R. C, Sodr√©"
- ". L. Azevedo, L.O."
- "Pessoa J & Equipe de Campo"
- "D.R. Gonzaga et collaborators"
```

Para esses casos, **regras lingu√≠sticas** apresentam baixa confian√ßa ou falham completamente. O modelo BERT NER analisa o contexto sem√¢ntico da string e extrai entidades nomeadas com alta precis√£o.

### Benef√≠cios

| Aspecto | Impacto |
|---------|---------|
| **Precis√£o** | Boost de confian√ßa de ~0.65 para ~0.82+ em casos complexos |
| **Recall** | Redu√ß√£o de ~50% em falsos negativos (entidades n√£o detectadas) |
| **Robustez** | Lida com varia√ß√µes ortogr√°ficas, abrevia√ß√µes n√£o padronizadas |
| **Escalabilidade** | Apenas ~0.5-2% dos 4.6M registros usam fallback (~23K-92K casos) |
| **Performance** | Overhead m√©dio de ~2s por caso (vs. <1ms das regras) |

---

## Arquitetura da Solu√ß√£o

### Fluxo de Decis√£o

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STRING DE ENTRADA                                        ‚îÇ
‚îÇ "Cc. Oliveira, L. S Inocencio, Mj. Silva"               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CLASSIFICADOR BASEADO EM REGRAS                          ‚îÇ
‚îÇ ‚Ä¢ Padr√µes regex (nomes, institui√ß√µes)                   ‚îÇ
‚îÇ ‚Ä¢ Heur√≠sticas lingu√≠sticas                              ‚îÇ
‚îÇ ‚Ä¢ Score: Confian√ßa = 0.62 < 0.70 ‚ùå                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ Confian√ßa?    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ       ‚îÇ
        ‚â•0.70‚îÇ       ‚îÇ<0.70
             ‚ñº       ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇOK  ‚îÇ  ‚îÇ ü§ñ BERT NER FALLBACK                 ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ ‚Ä¢ Carrega modelo em cache            ‚îÇ
                 ‚îÇ ‚Ä¢ Tokeniza string                    ‚îÇ
                 ‚îÇ ‚Ä¢ Infer√™ncia: extrai entidades PER   ‚îÇ
                 ‚îÇ ‚Ä¢ Reconstr√≥i classifica√ß√£o           ‚îÇ
                 ‚îÇ ‚Ä¢ Score: Confian√ßa = 0.84 ‚úÖ          ‚îÇ
                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚ñº
                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                         ‚îÇ Confian√ßa?    ‚îÇ
                         ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ       ‚îÇ
                        ‚â•0.70‚îÇ       ‚îÇ<0.70
                             ‚ñº       ‚ñº
                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                         ‚îÇOK  ‚îÇ  ‚îÇValueError‚îÇ
                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Componentes

```
src/pipeline/ner_fallback.py
‚îú‚îÄ‚îÄ NERFallback (classe principal)
‚îÇ   ‚îú‚îÄ‚îÄ __init__() ‚Üí Lazy loading do modelo BERT
‚îÇ   ‚îú‚îÄ‚îÄ _load_model() ‚Üí Carrega modelo + tokenizer (cache)
‚îÇ   ‚îú‚îÄ‚îÄ classify_with_ner() ‚Üí Entrada: string + confian√ßa
‚îÇ   ‚îÇ                          Sa√≠da: NEROutput
‚îÇ   ‚îî‚îÄ‚îÄ _extract_entities() ‚Üí L√≥gica de extra√ß√£o de entidades
‚îÇ
‚îî‚îÄ‚îÄ NEROutput (Pydantic model)
    ‚îú‚îÄ‚îÄ category: ClassificationCategory
    ‚îú‚îÄ‚îÄ confidence: float (0.70-1.0)
    ‚îú‚îÄ‚îÄ entities: List[str] (entidades extra√≠das)
    ‚îî‚îÄ‚îÄ reasoning: str (explica√ß√£o do modelo)
```

---

## Especifica√ß√µes do Modelo

### Modelo Selecionado

**Nome**: `marquesafonso/bertimbau-large-ner-selective` (BERTimbau-NER)

**Justificativa da Escolha**:

| Crit√©rio | An√°lise |
|----------|---------|
| **Idioma** | Portugu√™s brasileiro otimizado |
| **Tamanho** | ~1.3GB (large model, alto desempenho) |
| **Precis√£o** | F1-score ~97% para entidades PESSOA |
| **Infer√™ncia** | ~1-3s por string em GPU, ~2-5s em CPU |
| **Pr√©-treinamento** | Fine-tuned especificamente para NER em portugu√™s |
| **Arquitetura** | BERTimbau Large (neuralmind) com camada NER seletiva |
| **Acelera√ß√£o GPU** | Suporte CUDA para processamento em larga escala |
| **Cobertura** | 100% dos registros processados com NER |

### Alternativas Consideradas

| Modelo | Pr√≥s | Contras | Decis√£o |
|--------|------|---------|---------|
| `pierreguillou/bert-base-cased-pt-lenerbr` | Menor (~420MB), LeNER-Br | F1 ~96%, menos robusto | ‚ùå Rejeitado |
| `neuralmind/bert-base-portuguese-cased` | Menor (~390MB) | Requer fine-tuning para NER | ‚ùå Rejeitado |
| `neuralmind/bert-large-portuguese-cased` | BERTimbau base | Requer fine-tuning para NER | ‚ùå Rejeitado |
| `Davlan/bert-base-multilingual-cased-ner-hrl` | Multil√≠ngue | Menos espec√≠fico para PT-BR | ‚ùå Rejeitado |
| **`marquesafonso/bertimbau-large-ner-selective`** | **Melhor F1, fine-tuned, suporte GPU, 100% cobertura** | Maior tamanho | ‚úÖ **Selecionado** |

### Caracter√≠sticas do Modelo

```python
# Informa√ß√µes do modelo BERTimbau-NER
{
  "architecture": "BERT Large",
  "hidden_size": 1024,
  "num_hidden_layers": 24,
  "num_attention_heads": 16,
  "vocab_size": 29794,
  "max_position_embeddings": 512,
  "type_vocab_size": 2,

  # M√©tricas (fine-tuned para NER em portugu√™s)
  "f1_pessoa": 0.97,
  "precision_pessoa": 0.96,
  "recall_pessoa": 0.98,

  # R√≥tulos de entidades
  "labels": [
    "O",           # Outside (n√£o √© entidade)
    "B-PESSOA",    # Begin Person
    "I-PESSOA",    # Inside Person
    "B-ORGANIZACAO",  # Begin Organization
    "I-ORGANIZACAO",  # Inside Organization
    "B-LOCAL",     # Begin Location
    "I-LOCAL",     # Inside Location
    # ... (outros r√≥tulos espec√≠ficos do modelo)
  ]
}
```

---

## Integra√ß√£o no Pipeline

### Ponto de Integra√ß√£o

O NER fallback √© chamado **dentro do `Classifier`** (src/pipeline/classifier.py):

```python
# src/pipeline/classifier.py (trecho)
from src.pipeline.ner_fallback import NERFallback

class Classifier:
    def __init__(self, ner_fallback: Optional[NERFallback] = None):
        self.ner_fallback = ner_fallback

    def classify(self, input_data: ClassificationInput) -> ClassificationOutput:
        # 1. Tenta classifica√ß√£o por regras
        result = self._classify_with_rules(input_data.text)

        # 2. Se confian√ßa baixa E NER habilitado
        if result.confidence < 0.70 and self.ner_fallback:
            try:
                ner_result = self.ner_fallback.classify_with_ner(
                    text=input_data.text,
                    original_confidence=result.confidence
                )
                # Usa resultado do NER se melhorou confian√ßa
                if ner_result.confidence >= 0.70:
                    return ClassificationOutput(
                        category=ner_result.category,
                        confidence=ner_result.confidence,
                        reasoning=f"NER fallback: {ner_result.reasoning}"
                    )
            except Exception as e:
                # Fallback falhou, usa resultado original
                logger.warning(f"NER fallback error: {e}")

        # 3. Valida confian√ßa final
        if result.confidence < 0.70:
            raise ValueError(f"Confidence {result.confidence} below threshold")

        return result
```

### Schema de Dados

```python
# src/models/schemas.py (adi√ß√£o)
from pydantic import BaseModel, Field
from typing import List

class NEROutput(BaseModel):
    """Sa√≠da do fallback NER."""
    category: ClassificationCategory
    confidence: float = Field(ge=0.70, le=1.0)
    entities: List[str]  # Entidades extra√≠das (ex: ["Oliveira", "Silva"])
    reasoning: str       # Explica√ß√£o (ex: "Detected 2 PESSOA entities")
```

---

## Implementa√ß√£o T√©cnica

### Carregamento do Modelo (Lazy + Cache)

```python
# src/pipeline/ner_fallback.py
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
import torch

class NERFallback:
    AVAILABLE_MODELS = {
        "bertimbau-ner": "marquesafonso/bertimbau-large-ner-selective",  # Padr√£o
        "lenerbr": "pierreguillou/bert-base-cased-pt-lenerbr",
        # ... outros modelos
    }

    def __init__(self, device: Optional[str] = None, model_key: str = "bertimbau-ner"):
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_key = model_key
        self.model_name = self.AVAILABLE_MODELS.get(model_key)
        self.ner_pipeline = None

    def _load_model(self):
        """Lazy loading: carrega modelo apenas no primeiro uso."""
        if self.ner_pipeline is not None:
            return

        logger.info(f"Loading NER model: {self.model_name} on {self.device}")
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        model = AutoModelForTokenClassification.from_pretrained(self.model_name)

        # Move para GPU se dispon√≠vel
        if self.device == 'cuda':
            model = model.to('cuda')

        # Cria pipeline com aggregation strategy
        self.ner_pipeline = pipeline(
            "ner",
            model=model,
            tokenizer=tokenizer,
            device=0 if self.device == 'cuda' else -1,
            aggregation_strategy="simple"  # Merge subword tokens
        )
        logger.info("NER model loaded and cached")
```

**Benef√≠cios do Lazy Loading**:
- Modelo s√≥ √© carregado no primeiro uso
- Economiza ~2-3s no startup
- Mem√≥ria (~1.3GB) alocada apenas quando necess√°rio

### Extra√ß√£o de Entidades

```python
def _extract_entities(self, text: str) -> List[str]:
    """Extrai entidades PESSOA da string."""
    # 1. Tokeniza texto
    inputs = self._tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=512,
        padding=True
    )

    # 2. Infer√™ncia
    with torch.no_grad():
        outputs = self._model(**inputs)

    # 3. Pega logits e converte para labels
    predictions = torch.argmax(outputs.logits, dim=2)
    tokens = self._tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])

    # 4. Extrai apenas entidades PESSOA (B-PER, I-PER)
    entities = []
    current_entity = []

    for token, pred_id in zip(tokens, predictions[0]):
        label = self._model.config.id2label[pred_id.item()]

        if label == "B-PER":  # In√≠cio de pessoa
            if current_entity:
                entities.append("".join(current_entity).replace("##", ""))
            current_entity = [token]
        elif label == "I-PER":  # Continua√ß√£o de pessoa
            current_entity.append(token)
        else:  # Outra entidade ou n√£o-entidade
            if current_entity:
                entities.append("".join(current_entity).replace("##", ""))
                current_entity = []

    # Adiciona √∫ltima entidade se existir
    if current_entity:
        entities.append("".join(current_entity).replace("##", ""))

    return entities
```

### L√≥gica de Classifica√ß√£o

```python
def classify_with_ner(self, text: str, original_confidence: float) -> NEROutput:
    """Classifica string usando NER."""
    self._load_model()  # Lazy load

    # Timeout de 5s
    with timeout(5):
        entities = self._extract_entities(text)

    # L√≥gica de classifica√ß√£o
    num_entities = len(entities)

    if num_entities == 0:
        # Sem entidades pessoa ‚Üí pode ser institui√ß√£o ou grupo
        category = ClassificationCategory.NAO_DETERMINADO
        confidence = 0.60
        reasoning = "No PESSOA entities detected by NER"

    elif num_entities == 1:
        category = ClassificationCategory.PESSOA
        confidence = 0.85
        reasoning = f"Single PESSOA entity: {entities[0]}"

    else:  # num_entities >= 2
        category = ClassificationCategory.CONJUNTO_PESSOAS
        confidence = 0.90
        reasoning = f"Multiple PESSOA entities: {', '.join(entities)}"

    return NEROutput(
        category=category,
        confidence=confidence,
        entities=entities,
        reasoning=reasoning
    )
```

---

## Performance e Otimiza√ß√£o

### Benchmarks

**Hardware**: CPU Intel i7-12700 (12 cores), 16GB RAM

| M√©trica | Valor | Observa√ß√£o |
|---------|-------|------------|
| **Carregamento do modelo** | ~2.5s | Uma vez por execu√ß√£o |
| **Infer√™ncia (string curta <50 chars)** | ~1.2s | Maioria dos casos |
| **Infer√™ncia (string m√©dia 50-150 chars)** | ~2.0s | Casos complexos |
| **Infer√™ncia (string longa >150 chars)** | ~3.5s | Truncado em 512 tokens |
| **Mem√≥ria (modelo em RAM)** | ~1.3GB | Cache persistente (BERTimbau Large) |

### Impacto no Pipeline Total

**Cen√°rio**: 4.6M registros, 100% processados com NER

```
Com NER Sequencial (abordagem atual):
- Throughput: ~43-151 rec/s (dependendo do tamanho do batch de teste)
- GPU utilizado: 100%
- Processamento: Sequential por design (melhor performance)
- Warning de GPU: "You seem to be using the pipelines sequentially on GPU" ‚Üí IGNORAR (harmless)
```

### Otimiza√ß√µes Implementadas

#### 1. Lazy Loading
```python
# Modelo carregado apenas no primeiro uso
if self._model is None:
    self._load_model()
```

#### 2. GPU Acceleration
```python
# Uso autom√°tico de GPU se dispon√≠vel
device = 'cuda' if torch.cuda.is_available() else 'cpu'
ner_pipeline = pipeline("ner", model=model, device=0 if device == 'cuda' else -1)
```

#### 3. Cache de Modelo em Mem√≥ria
```python
# Modelo compartilhado via lazy loading
# Uma √∫nica inst√¢ncia carregada persiste durante toda a execu√ß√£o
```

### Experimento: GPU Batch Processing (REVERTIDO)

**Objetivo**: Eliminar warning de GPU e melhorar throughput com batch processing

**Data**: 2025-10-05

**Tentativas realizadas**:

1. **Batch Acumula√ß√£o Manual** (batch_size=32):
   - Throughput: 54 rec/s
   - Resultado: 3x MAIS LENTO ‚ùå

2. **Batch Acumula√ß√£o Manual** (batch_size=128):
   - Throughput: 27 rec/s
   - Resultado: 5.5x MAIS LENTO ‚ùå

3. **Dataset + KeyDataset** (Hugging Face):
   - Throughput: 48 rec/s
   - Resultado: 3x MAIS LENTO ‚ùå

**Compara√ß√£o de Performance**:

| Abordagem | Throughput | vs. Original |
|-----------|------------|--------------|
| **Original (sequential)** | **151 rec/s** | **Baseline** |
| Batch manual (32) | 54 rec/s | -64% ‚ùå |
| Batch manual (128) | 27 rec/s | -82% ‚ùå |
| Dataset + KeyDataset | 48 rec/s | -68% ‚ùå |

**An√°lise da Causa**:

- **Python Overhead > GPU Gains**: Cria√ß√£o de Dataset, acumula√ß√£o de batches, e overhead de fun√ß√£o adicionaram lat√™ncia significativa
- **Batching n√£o √© sempre melhor**: Para este pipeline, o overhead de Python superou os ganhos de efici√™ncia de GPU
- **Sequential √© mais simples e r√°pido**: Processamento record-by-record provou ser 3x mais r√°pido

**Li√ß√µes Aprendidas**:

1. ‚ö†Ô∏è **Warning de GPU pode ser ignorado**: O warning "You seem to be using the pipelines sequentially on GPU" √© informativo, n√£o cr√≠tico
2. üöÄ **Simpler is better**: Implementa√ß√£o sequential superou tentativas de otimiza√ß√£o complexas
3. üìä **Measure, don't assume**: Sempre benchmarque antes de "otimizar"
4. üîÑ **Python overhead matters**: Dataset creation e batch accumulation t√™m custo n√£o trivial

**Decis√£o Final**: ‚úÖ MANTER PROCESSAMENTO SEQUENTIAL

**C√≥digo revertido via**:
```bash
git checkout src/cli.py
git checkout src/pipeline/classifier.py
git checkout src/pipeline/ner_fallback.py
```

**Arquivos modificados e revertidos**:
- `src/cli.py`: Removido batch accumulation logic
- `src/pipeline/classifier.py`: Removido classify_batch()
- `src/pipeline/ner_fallback.py`: Removido batch NER methods
- `config.yaml`: Removido ner_batch_size parameter
- `src/config.py`: Removido ner_batch_size field
- `requirements-ner.txt`: Removido datasets dependency

---

## Configura√ß√£o e Uso

### Configura√ß√£o via `config.yaml`

**Nota**: A configura√ß√£o atual n√£o possui se√ß√£o `ai:` dedicada. O NER √© habilitado por padr√£o e usa o modelo BERTimbau-NER.

```yaml
# config.yaml atual (sem se√ß√£o AI - NER sempre habilitado)
processing:
  batch_size: 10000
  workers: 8
  confidence_threshold: 0.70
```

**Configura√ß√£o futura proposta** (para permitir customiza√ß√£o):

```yaml
ai:
  # Habilitar fallback NER (padr√£o: true)
  enable_ner: true

  # Modelo a usar (padr√£o: bertimbau-ner)
  ner_model_key: "bertimbau-ner"  # ou "lenerbr", "bertimbau-base", etc.

  # Device (auto-detect por padr√£o)
  device: "cuda"  # ou "cpu", ou null para auto

  # Cache local do modelo (diret√≥rio)
  cache_dir: "./models/cache"
```

### Uso Program√°tico

```python
from src.pipeline.classifier import Classifier
from src.pipeline.ner_fallback import NERFallback
from src.models.schemas import ClassificationInput

# Inicializa NER fallback com modelo padr√£o (BERTimbau-NER)
ner = NERFallback(model_key="bertimbau-ner", device="cuda")

# Inicializa classificador com NER
classifier = Classifier(ner_fallback=ner)

# Processa string complexa
result = classifier.classify(
    ClassificationInput(text="Cc. Oliveira, L. S Inocencio, Mj. Silva")
)

print(f"Category: {result.category}")
print(f"Confidence: {result.confidence}")
print(f"Reasoning: {result.reasoning}")

# Output esperado:
# Category: CONJUNTO_PESSOAS
# Confidence: 0.90
# Reasoning: NER extracted 3 PESSOA entities: Oliveira, Inocencio, Silva
```

### Uso via CLI

```bash
# Processar com NER habilitado (padr√£o - sempre habilitado na vers√£o atual)
python src/cli.py --config config.yaml

# Processar com limite de registros (teste)
python src/cli.py --config config.yaml --max-records 1000

# Modo verbose (mostra progresso detalhado)
python src/cli.py --config config.yaml --verbose
```

**Nota**: Na implementa√ß√£o atual, o NER est√° sempre habilitado. N√£o h√° flag `--no-ai` dispon√≠vel.

---

## M√©tricas e Monitoramento

### M√©tricas Coletadas

```python
# src/pipeline/ner_fallback.py (adi√ß√£o)
class NERMetrics:
    total_invocations: int = 0
    total_time: float = 0.0
    confidence_boosts: List[Tuple[float, float]] = []  # (antes, depois)

    def log_invocation(self, before: float, after: float, elapsed: float):
        self.total_invocations += 1
        self.total_time += elapsed
        self.confidence_boosts.append((before, after))

    def summary(self) -> dict:
        avg_boost = sum(after - before for before, after in self.confidence_boosts) / len(self.confidence_boosts)
        return {
            "invocations": self.total_invocations,
            "total_time": self.total_time,
            "avg_time_per_call": self.total_time / self.total_invocations,
            "avg_confidence_boost": avg_boost
        }
```

### Logging Detalhado

```python
# Modo verbose
[INFO] BERT model loaded: pierreguillou/bert-base-cased-pt-lenerbr
[DEBUG] NER invoked for: "Cc. Oliveira, L. S Inocencio, Mj. Silva"
[DEBUG] Original confidence: 0.62
[DEBUG] Extracted entities: ['Oliveira', 'Inocencio', 'Silva']
[DEBUG] NER confidence: 0.90 (boost: +0.28)
[DEBUG] Inference time: 1.85s
```

### Dashboard (CLI Output)

```bash
python src/cli.py --config config.yaml --verbose

[INFO] Processing 4,600,000 records...
[INFO] BERT model loading... done (2.3s)
[PROGRESS] ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 97% | 4.46M/4.6M | 210 rec/s

=== NER Fallback Statistics ===
Total invocations:        12,450
Percentage of records:    0.27%
Total NER time:           24,900s (~6.9h)
Avg time per call:        2.00s
Avg confidence boost:     +0.18 (0.65 ‚Üí 0.83)
Success rate (‚â•0.70):     94.2%
```

---

## Troubleshooting

### Problemas Comuns

#### 1. Modelo n√£o carrega

**Sintoma**:
```
OSError: Can't load tokenizer for 'marquesafonso/bertimbau-large-ner-selective'
```

**Solu√ß√µes**:
```bash
# For√ßa download do modelo BERTimbau-NER (~1.3GB)
python -c "from transformers import AutoTokenizer; AutoTokenizer.from_pretrained('marquesafonso/bertimbau-large-ner-selective')"

# Verifica conex√£o com Hugging Face
curl -I https://huggingface.co

# Define cache local (para n√£o baixar m√∫ltiplas vezes)
export TRANSFORMERS_CACHE=./models/cache

# Verifica espa√ßo em disco (modelo requer ~1.5GB livres)
df -h .
```

#### 2. Timeout frequente

**Sintoma**:
```
TimeoutError: NER inference exceeded 5s timeout
```

**Solu√ß√µes**:
```yaml
# config.yaml - Aumenta timeout
ai:
  ner_timeout: 10  # Aumenta para 10s
```

```python
# Ou: Trunca strings longas antes de enviar para NER
text = text[:300]  # Limita a 300 caracteres
```

#### 3. Mem√≥ria insuficiente

**Sintoma**:
```
RuntimeError: CUDA out of memory
MemoryError: Unable to allocate tensor
```

**An√°lise**:
- BERTimbau-NER Large requer ~1.3GB em RAM (CPU) ou ~2GB em VRAM (GPU)
- Processamento sequential adiciona ~500MB de overhead

**Solu√ß√µes**:
```python
# For√ßa uso de CPU ao inv√©s de GPU
ner = NERFallback(device="cpu")

# Ou verifica mem√≥ria dispon√≠vel antes de inicializar
import psutil
if psutil.virtual_memory().available < 2 * 1024**3:
    raise RuntimeError("Insufficient RAM (<2GB available)")
```

```yaml
# Reduz batch size se mem√≥ria continuar insuficiente
processing:
  batch_size: 5000  # Reduz de 10000 para 5000
```

#### 4. Confian√ßa ainda baixa ap√≥s NER

**Sintoma**:
```
ValueError: Confidence 0.68 below threshold (0.70)
```

**An√°lise**:
- NER n√£o detectou entidades PESSOA na string
- Pode ser genuinamente NAO_DETERMINADO ou GRUPO_PESSOAS

**Solu√ß√£o**:
```python
# Permite confian√ßa ligeiramente menor para NER fallback
if result.confidence >= 0.65 and ner_was_used:
    # Aceita com warning
    logger.warning(f"Low NER confidence: {result.confidence}")
```

---

## Refer√™ncias

### Modelos

- **Modelo Principal**: [marquesafonso/bertimbau-large-ner-selective](https://huggingface.co/marquesafonso/bertimbau-large-ner-selective) - BERTimbau-NER Large
- **BERTimbau Base**: [neuralmind/bert-base-portuguese-cased](https://huggingface.co/neuralmind/bert-base-portuguese-cased)
- **BERTimbau Large**: [neuralmind/bert-large-portuguese-cased](https://huggingface.co/neuralmind/bert-large-portuguese-cased)
- **LeNER-Br Model**: [pierreguillou/bert-base-cased-pt-lenerbr](https://huggingface.co/pierreguillou/bert-base-cased-pt-lenerbr)

### Datasets e Papers

- **Dataset LeNER-Br**: [LeNER-Br: a Dataset for Named Entity Recognition in Brazilian Legal Text](https://cic.unb.br/~teodecampos/LeNER-Br/)
- **Paper BERT**: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- **BERTimbau Paper**: [BERTimbau: Portuguese BERT](https://arxiv.org/abs/2008.07869)

### Bibliotecas

- **Transformers**: [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers/)
- **PyTorch**: [PyTorch Documentation](https://pytorch.org/docs/)
- **Tokenizers**: [Hugging Face Tokenizers](https://huggingface.co/docs/tokenizers/)

### Artigos Relacionados

- [Named Entity Recognition in Portuguese](https://www.scielo.br/pdf/prc/v32n2/1678-7153-prc-32-02-e3229.pdf)
- [Deep Learning for NER in Scientific Texts](https://arxiv.org/abs/1904.10503)

---

## Hist√≥rico de Vers√µes

| Vers√£o | Data | Mudan√ßas |
|--------|------|----------|
| 1.0.0 | 2025-10-03 | Integra√ß√£o inicial do NER com BERTimbau-NER Large |
| 1.0.1 | 2025-10-05 | Experimento batch processing (revertido ap√≥s benchmarks) |
| 1.0.2 | 2025-10-05 | Documenta√ß√£o t√©cnica completa com lessons learned |
| 2.0.0 | TBD | Fine-tuning com dados espec√≠ficos de herb√°rios |
| 3.0.0 | TBD | API configur√°vel para sele√ß√£o de modelos via config.yaml |

---

**√öltima atualiza√ß√£o**: 2025-10-05
**Autor**: Sistema de Identifica√ß√£o de Coletores
**Licen√ßa**: MIT
